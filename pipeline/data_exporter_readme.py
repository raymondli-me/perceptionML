#!/usr/bin/env python3
"""Comprehensive README generation for data exports with full code references."""

from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional
import json
import sys

# Import torch to check CUDA availability
try:
    import torch
except ImportError:
    torch = None


def generate_comprehensive_readme(output_dir: Path, pipeline: Any, results: Dict[str, Any],
                                config: Any, exclude_text: bool, anonymize_ids: bool, 
                                cli_command: Optional[str] = None, versions: Dict[str, str] = None) -> str:
    """Generate comprehensive README with code references for every fact."""
    
    # Get sampling info
    sampling_info = getattr(pipeline.data_loader, 'sampling_info', None)
    
    readme_content = f"""# PerceptionML Pipeline Export Documentation

**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**Pipeline Version:** {getattr(pipeline, 'version', '1.0.0')}

## Table of Contents

1. [Overview](#overview)
2. [Command and Configuration](#command-and-configuration)
3. [Privacy and Data Handling](#privacy-and-data-handling)
4. [Sampling Details](#sampling-details)
5. [Directory Structure and File Descriptions](#directory-structure-and-file-descriptions)
6. [Complete Algorithm Documentation](#complete-algorithm-documentation)
7. [Data Processing Pipeline](#data-processing-pipeline)
8. [Model Training Details](#model-training-details)
9. [Export Implementation Details](#export-implementation-details)
10. [Reproducibility Guide](#reproducibility-guide)
11. [Code References](#code-references)

---

## Overview

This directory contains a complete export of all data generated by the PerceptionML text analysis pipeline.
The pipeline processes text data through multiple stages of analysis to identify patterns, relationships,
and causal effects between outcomes.

### Key Components Exported

1. **Raw Data and Embeddings** - Original input data and text embeddings
2. **Dimensionality Reduction** - PCA and UMAP results  
3. **Clustering Results** - Topic assignments and keywords
4. **Feature Importance** - PC selection metrics and SHAP values
5. **Causal Analysis** - DML estimates, residuals, and predictions
6. **Visualization Data** - Pre-processed data for 3D visualization

---

## Command and Configuration

### Command Used

```bash
{cli_command or 'python run_pipeline.py [command not captured]'}
```

**Implementation:** Command captured in `run_pipeline.py` lines 666-667:
```python
import sys
pipeline._cli_command = ' '.join(sys.argv)
```

### Configuration Details

The pipeline configuration is stored in `metadata.json` and includes all parameters used:

```json
{json.dumps(config.to_dict(), indent=2)}
```

**Implementation:** Configuration handling in `pipeline/config.py`:
- `PipelineConfig` class (lines 66-175)
- `DataConfig` class (lines 31-60)
- `AnalysisConfig` class (lines 62-90)

---

## Privacy and Data Handling

### Privacy Settings Applied

- **Text Excluded:** {'Yes' if exclude_text else 'No'}
  - Implementation: `data_exporter.py` lines 206-210
  ```python
  if exclude_text:
      data_export = data.drop(columns=[self.config.data.text_column])
  else:
      data_export = data
  ```

- **IDs Anonymized:** {'Yes' if anonymize_ids else 'No'}
  - Implementation: `data_exporter.py` lines 194-203
  ```python
  if anonymize_ids:
      original_ids = data[self.config.data.id_column].values
      data[self.config.data.id_column] = range(len(data))
      id_mapping = pd.DataFrame({{
          'anonymous_id': range(len(data)),
          'original_id_hash': [hashlib.sha256(str(id).encode()).hexdigest()[:16] 
                             for id in original_ids]
      }})
  ```

### Data Handling Process

1. **Text Column Handling**
   - If excluded: Column dropped before export (line 208)
   - If included: Full text preserved in export

2. **ID Anonymization Process**
   - Original IDs replaced with sequential integers (line 196)
   - SHA-256 hash (first 16 chars) stored for reference (line 200-201)
   - Mapping file created: `id_mapping.csv`

---

{f'''## Sampling Details

### Sampling Configuration

**Implementation:** Sampling logic in `pipeline/data_loader.py` lines 150-207

- **Sampling Applied:** Yes
- **Method:** {sampling_info['method']}
- **Original Dataset Size:** {sampling_info['original_size']:,} records
- **Sample Size:** {sampling_info['sample_size']:,} records ({sampling_info['sample_ratio']:.1%} of original)
- **Random Seed:** {sampling_info['sample_seed']}
{f"- **Stratification Column:** {sampling_info['stratify_column']}" if sampling_info.get('stratify_column') else ""}

### Sampling Implementation

The sampling process is implemented in `DataLoader._apply_sampling()` (lines 150-207):

```python
def _apply_sampling(self):
    if self.config.data.sample_seed is not None:
        np.random.seed(self.config.data.sample_seed)
        random.seed(self.config.data.sample_seed)
    
    # Try stratified sampling first
    stratify_column = self._get_stratify_column()
    if stratify_column:
        from sklearn.model_selection import train_test_split
        _, sampled_data = train_test_split(
            self.data,
            test_size=self.config.data.sample_size,
            stratify=self.data[stratify_column],
            random_state=self.config.data.sample_seed
        )
```

### Sampling Rationale

Datasets larger than 10,000 records can cause:
- **HTML file size issues** - Browser performance degrades with large point clouds
- **Memory constraints** - Embedding generation requires ~4GB per 10k samples
- **Processing time** - UMAP scales as O(n^1.14) for n samples

The sampling preserves statistical properties through:
{f"- Stratification on '{sampling_info['stratify_column']}' to maintain outcome distribution" if sampling_info.get('stratify_column') else "- Random sampling with fixed seed for reproducibility"}
''' if sampling_info else '''## Sampling Details

**No sampling applied** - Processing full dataset
'''}

---

## Directory Structure and File Descriptions

### 01_raw_data/

Contains original input data and text embeddings.

#### Files Generated

{f'''**sampled_data.csv** - The sampled subset used for analysis
- Implementation: `data_exporter.py` lines 212-215
- Columns: {', '.join(pipeline.data_loader.data.columns.tolist()) if hasattr(pipeline, 'data_loader') else 'N/A'}
- Rows: {len(pipeline.data_loader.data) if hasattr(pipeline, 'data_loader') else 'N/A'}

**original_data_full.csv** - Complete dataset before sampling  
- Implementation: `data_exporter.py` lines 221-243
- Same columns as sampled_data.csv
- Rows: {len(pipeline.data_loader.original_data) if hasattr(pipeline.data_loader, 'original_data') and pipeline.data_loader.original_data is not None else 'N/A'}
''' if sampling_info else f'''**original_data.csv** - The complete dataset
- Implementation: `data_exporter.py` lines 217-218
- Columns: {', '.join(pipeline.data_loader.data.columns.tolist()) if hasattr(pipeline, 'data_loader') else 'N/A'}
- Rows: {len(pipeline.data_loader.data) if hasattr(pipeline, 'data_loader') else 'N/A'}
'''}

**embeddings.csv** - Text embeddings from {config.embedding_model}
- Implementation: `data_exporter.py` lines 245-251
- Generation: `pipeline/embeddings.py` `generate_embeddings()` method
- Shape: [{pipeline.embedding_gen.embeddings.shape[0] if hasattr(pipeline, 'embedding_gen') and hasattr(pipeline.embedding_gen, 'embeddings') and pipeline.embedding_gen.embeddings is not None else 'N/A'} × {pipeline.embedding_gen.embeddings.shape[1] if hasattr(pipeline, 'embedding_gen') and hasattr(pipeline.embedding_gen, 'embeddings') and pipeline.embedding_gen.embeddings is not None else 'N/A'}]
- Columns: {config.data.id_column}, emb_0, emb_1, ..., emb_{pipeline.embedding_gen.embeddings.shape[1]-1 if hasattr(pipeline, 'embedding_gen') and hasattr(pipeline.embedding_gen, 'embeddings') and pipeline.embedding_gen.embeddings is not None else 'N'}

#### Embedding Generation Process

**Implementation:** `pipeline/embeddings.py` lines 50-108

```python
def generate_embeddings(self, texts: List[str]) -> np.ndarray:
    # Model loading (lines 31-36)
    self.model = SentenceTransformer(self.config.embedding_model)
    
    # Batch processing (lines 71-84)
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i + batch_size]
        batch_embeddings = self.model.encode(
            batch_texts,
            normalize_embeddings=True,
            show_progress_bar=False
        )
```

Key parameters:
- **Normalization:** L2 normalization applied (line 77)
- **Batch size:** {config.analysis.batch_size}
- **Max length:** {config.analysis.max_text_length} tokens
- **Device:** {'CUDA' if torch and torch.cuda.is_available() else 'CPU'}

---

### 02_dimensionality_reduction/

Results from PCA and UMAP dimensionality reduction.

#### PCA Files

**pca_features.csv** - Principal component scores
- Implementation: `data_exporter.py` lines 273-275
- Generation: `pipeline/dimensionality.py` `fit_pca()` method (lines 31-75)
- Shape: [{len(pipeline.data_loader.data) if hasattr(pipeline, 'data_loader') else 'N/A'} × {config.analysis.pca_components}]
- Columns: {config.data.id_column}, PC0, PC1, ..., PC{config.analysis.pca_components-1}

**pca_explained_variance.csv** - Variance explained by each PC
- Implementation: `data_exporter.py` lines 277-283
- Columns: PC, explained_variance_ratio, cumulative_variance_ratio
- Total components: {config.analysis.pca_components}

**pca_percentiles.csv** - Percentile rank of each sample on each PC
- Implementation: `data_exporter.py` lines 285-289
- Calculation: `scipy.stats.rankdata(pc_values, 'average') / n * 100`
- Use case: Understanding relative positions in PC space

**pca_loadings.csv** - Component loadings (if available)
- Implementation: `data_exporter.py` lines 291-295
- Shape: [{pipeline.embedding_gen.embeddings.shape[1] if hasattr(pipeline, 'embedding_gen') and hasattr(pipeline.embedding_gen, 'embeddings') and pipeline.embedding_gen.embeddings is not None else 'N/A'} × {config.analysis.pca_components}]
- Interpretation: How each embedding dimension contributes to each PC

#### PCA Implementation Details

**Algorithm:** `pipeline/dimensionality.py` lines 31-75

```python
def fit_pca(self, embeddings: np.ndarray) -> Dict[str, np.ndarray]:
    # Standardization (lines 38-40)
    self.scaler = StandardScaler()
    embeddings_scaled = self.scaler.fit_transform(embeddings)
    
    # PCA fitting (lines 43-47)
    self.pca = PCA(
        n_components=self.config.analysis.pca_components,
        random_state=42
    )
    pca_features = self.pca.fit_transform(embeddings_scaled)
    
    # Percentile calculation (lines 61-66)
    percentiles = np.zeros_like(pca_features)
    for i in range(pca_features.shape[1]):
        percentiles[:, i] = rankdata(pca_features[:, i], 'average') / len(pca_features) * 100
```

**Key Parameters:**
- Preprocessing: StandardScaler (mean=0, std=1)
- Components: {config.analysis.pca_components}
- Random state: 42
- Solver: Automatically selected by scikit-learn (likely SVD)

#### UMAP Files

**umap_3d.csv** - 3D UMAP coordinates
- Implementation: `data_exporter.py` lines 314-317
- Generation: `pipeline/dimensionality.py` `fit_umap()` method (lines 77-120)
- Columns: {config.data.id_column}, x, y, z
- Coordinate range: [-1, 1] (normalized)

#### UMAP Implementation Details

**Algorithm:** `pipeline/dimensionality.py` lines 77-120

```python
def fit_umap(self, features: np.ndarray) -> Dict[str, np.ndarray]:
    # UMAP configuration (lines 84-92)
    self.umap_model = umap.UMAP(
        n_components=self.config.analysis.umap_dimensions,
        n_neighbors=self.config.analysis.umap_n_neighbors,
        min_dist=self.config.analysis.umap_min_dist,
        metric='cosine',
        random_state=42,
        verbose=True
    )
    
    # Fitting (line 94)
    umap_embeddings = self.umap_model.fit_transform(features)
    
    # Normalization (lines 104-107)
    for i in range(umap_embeddings.shape[1]):
        coord_min = umap_embeddings[:, i].min()
        coord_max = umap_embeddings[:, i].max()
        normalized[:, i] = 2 * (umap_embeddings[:, i] - coord_min) / (coord_max - coord_min) - 1
```

**UMAP Parameters:**
- **n_neighbors:** {config.analysis.umap_n_neighbors}
  - Controls local vs global structure (lower = more local detail)
- **min_dist:** {config.analysis.umap_min_dist}
  - Minimum distance between points (lower = tighter clumps)
- **metric:** cosine
  - Appropriate for high-dimensional normalized embeddings
- **Output normalization:** Linear scaling to [-1, 1]

---

### 03_clustering/

Topic modeling and clustering results using HDBSCAN.

#### Files Generated

**cluster_assignments.csv** - Cluster/topic assignment for each sample
- Implementation: `data_exporter.py` lines 331-341
- Columns: {config.data.id_column}, cluster_id, topic_keywords
- Cluster -1: Noise points (not assigned to any cluster)

**topic_summary.csv** - Summary statistics for each topic
- Implementation: `data_exporter.py` lines 343-346
- Generated by: `pipeline/clustering.py` `prepare_topic_visualization()`
- Columns: topic_id, size, keywords, centroid_x, centroid_y, centroid_z

**topic_extreme_stats.csv** - Topic distribution in extreme outcome groups
- Implementation: `data_exporter.py` lines 348-367
- Generated by: `calculate_extreme_group_statistics()` (lines 276-363)
- Shows over/under-representation in high/low outcome groups

#### HDBSCAN Implementation

**Algorithm:** `pipeline/clustering.py` lines 33-72

```python
def fit_clusters(self, embeddings: np.ndarray) -> np.ndarray:
    # HDBSCAN configuration (lines 40-48)
    self.clustering_model = hdbscan.HDBSCAN(
        min_cluster_size=self.config.analysis.hdbscan_min_cluster_size,
        min_samples=self.config.analysis.hdbscan_min_samples,
        metric='euclidean',
        cluster_selection_epsilon=0.0,
        cluster_selection_method='eom'
    )
    
    # Fitting (line 50)
    cluster_labels = self.clustering_model.fit_predict(embeddings)
```

**Parameters:**
- **min_cluster_size:** {config.analysis.hdbscan_min_cluster_size}
- **min_samples:** {config.analysis.hdbscan_min_samples}
- **metric:** Euclidean (on UMAP coordinates)
- **selection_method:** EOM (Excess of Mass)

#### Topic Keyword Extraction

**Implementation:** `pipeline/clustering.py` lines 74-125

```python
def extract_topics(self, texts: List[str], cluster_labels: np.ndarray) -> Dict[int, str]:
    # c-TF-IDF calculation (lines 91-108)
    for cluster_id in unique_clusters:
        cluster_texts = [texts[i] for i in range(len(texts)) if cluster_labels[i] == cluster_id]
        cluster_text = ' '.join(cluster_texts)
        
        # TF-IDF vectorization
        tfidf = TfidfVectorizer(
            max_features=100,
            stop_words='english',
            ngram_range=(1, 2)
        )
```

**Process:**
1. Concatenate all texts in cluster
2. Apply TF-IDF with English stop words
3. Select top 10 keywords by TF-IDF score
4. Format as "word1 - word2 - word3..."

---

### 04_feature_importance/

Feature importance metrics from multiple methods.

#### Files Generated

**feature_importance_summary.csv** - PC importance by different methods
- Implementation: `data_exporter.py` lines 372-389
- Columns: PC, MI_score, MI_rank, XGBoost_importance, XGBoost_rank
- All {config.analysis.pca_components} PCs included

**pc_selection_summary.csv** - Top 5 PCs selected by each method
- Implementation: `data_exporter.py` lines 391-399
- Shows consensus/disagreement between methods

**shap_values_[outcome].csv** - SHAP values for each PC's contribution
- Implementation: `data_exporter.py` lines 402-410
- One file per outcome variable
- Columns: {config.data.id_column}, PC0_shap, PC1_shap, ..., PC{config.analysis.pca_components-1}_shap

#### Feature Selection Implementation

**Mutual Information:** `pipeline/dimensionality.py` lines 180-190

```python
# MI calculation
mi_scores = mutual_info_regression(
    pca_features, 
    combined_outcome,
    n_neighbors=3,
    random_state=42
)
```

**XGBoost Importance:** `pipeline/dimensionality.py` lines 193-213

```python
# XGBoost model for feature importance
xgb_model = xgb.XGBRegressor(
    n_estimators=self.config.analysis.xgb_n_estimators,
    max_depth=6,  # Fixed for feature selection
    random_state=42,
    tree_method='hist',
    device='cuda'
)
xgb_model.fit(pca_features, combined_outcome)
importance_scores = xgb_model.feature_importances_
```

**SHAP Calculation:** `pipeline/dml_analysis.py` lines 314-332

```python
def _calculate_contributions(self, X: np.ndarray, Y: np.ndarray, 
                           feature_names: List[str]) -> np.ndarray:
    model = xgb.XGBRegressor(...)
    model.fit(X, Y)
    
    # Feature importance weighted contributions
    importances = model.feature_importances_
    contributions = X * importances
```

---

### 05_dml_analysis/

Double Machine Learning causal inference results.

#### Main Files

**dml_effects_summary.csv** - All causal effect estimates
- Implementation: `data_exporter.py` lines 420-481
- Columns: effect, model, theta, se, ci_lower, ci_upper, pval, r2_y, r2_x, reduction
- Models included:
  - Naive OLS (no controls)
  - DML - Embeddings (all {pipeline.embedding_gen.embeddings.shape[1] if hasattr(pipeline, 'embedding_gen') and hasattr(pipeline.embedding_gen, 'embeddings') and pipeline.embedding_gen.embeddings is not None else 'N/A'} dimensions)
  - DML - 200 PCs (all PCs)
  - DML - Top 5 PCs (selected by XGBoost)

**model_diagnostics.csv** - R² and model fit statistics
- Implementation: `data_exporter.py` lines 507-520
- Cross-validated R² for each outcome

#### Residuals Directory

**Purpose:** First-stage residuals from DML cross-fitting procedure

For each outcome pair and model type, contains:
- `residuals_[treatment]_to_[outcome]_[model].csv`
- Model suffixes: `embeddings`, `200pcs`, `top5pcs`

**Implementation:** `data_exporter.py` lines 486-494

```python
residuals_df = pd.DataFrame({{
    self.config.data.id_column: pipeline.data_loader.data[self.config.data.id_column],
    'residual_outcome': residuals_dict['outcome'],
    'residual_treatment': residuals_dict['treatment']
}})
```

**Generation:** `pipeline/dml_analysis.py` lines 189-285 (cross-fitting)

```python
# Cross-fitting loop (lines 204-244)
for fold_idx, (train_idx, test_idx) in enumerate(kf.split(X)):
    # Train on fold
    model_outcome.fit(X_train, outcome_train)
    model_treatment.fit(X_train, treatment_train)
    
    # Predict on test fold
    pred_outcome = model_outcome.predict(X_test)
    pred_treatment = model_treatment.predict(X_test)
    
    # Calculate residuals
    residuals_outcome_all[test_idx] = outcome_test - pred_outcome
    residuals_treatment_all[test_idx] = treatment_test - pred_treatment
```

#### Predictions Directory

**Purpose:** First-stage predictions from nuisance models

For each outcome pair and model type:
- `predictions_[treatment]_to_[outcome]_[model].csv`
- Same model suffixes as residuals

**Implementation:** `data_exporter.py` lines 497-505

**Use Cases:**
1. Verify residuals: `actual - predicted = residual`
2. Assess first-stage model fit
3. Diagnostic plots (predicted vs actual)

#### DML Algorithm Details

**Implementation:** `pipeline/dml_analysis.py`

**1. Naive Estimation** (lines 104-130):
```python
def _estimate_naive(self, treatment: np.ndarray, outcome: np.ndarray) -> Dict:
    X = sm.add_constant(treatment)
    model = sm.OLS(outcome, X).fit()
    theta = model.params[1]
    se = model.bse[1]
```

**2. Cross-Fitted DML** (lines 189-285):
- K-fold cross-validation: {config.analysis.dml_n_folds} folds
- Nuisance models: XGBoost regressors
- Orthogonalization: Residualize both Y and X
- Final estimate: `theta = Σ(r_x * r_y) / Σ(r_x²)`

**3. Standard Error Calculation** (lines 249-252):
```python
se = np.sqrt(np.sum((residuals_outcome_all - theta * residuals_treatment_all) ** 2) / 
             (n * np.sum(residuals_treatment_all ** 2)))
```

**4. Model Storage** (lines 91-96):
```python
# Store with model suffix for multi-model export
storage_key = f"{{pair_key}}_{{model_suffix}}" if model_suffix else pair_key
if 'residuals' in dml_result:
    self.residuals[storage_key] = dml_result['residuals']
```

---

### 06_pc_analysis/

Detailed PC-level analysis and effects.

#### Files Generated

**pc_global_effects.csv** - Probability changes for extreme PC values
- Implementation: `data_exporter.py` lines 524-533
- For each PC: P(outcome|PC=90th percentile) vs P(outcome|PC=10th percentile)
- Generated by: `run_pipeline.py` `_calculate_pc_global_effects()` (lines 282-337)

**pc_detailed_stats.csv** - Comprehensive PC statistics
- Implementation: `data_exporter.py` lines 535-559
- Includes: rankings, correlations, SHAP statistics
- Generated by: `_calculate_pc_detailed_stats()` (lines 339-558)

**pc_topic_associations.csv** - Statistical tests of PC-topic relationships
- Implementation: `data_exporter.py` lines 561-573
- Welch's t-test comparing PC values for topic members vs non-members
- Effect sizes and p-values included

#### PC Effect Calculation

**Implementation:** `run_pipeline.py` lines 282-337

```python
def _calculate_pc_global_effects(self, pca_features: np.ndarray,
                               outcome_data: Dict[str, np.ndarray],
                               thresholds: Dict) -> Dict:
    # Standardize PCs
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(pca_features)
    
    # For each PC
    for pc_idx in range(pca_features.shape[1]):
        # Create test points at 90th and 10th percentiles
        test_data = np.zeros((2, pca_features.shape[1]))
        test_data[0, pc_idx] = np.percentile(pca_features[:, pc_idx], 90)
        test_data[1, pc_idx] = np.percentile(pca_features[:, pc_idx], 10)
        
        # Fit logistic regression and predict
        model_high = LogisticRegression(max_iter=1000, random_state=42)
        model_high.fit(X_scaled, y_high)
        prob_high_if_high = model_high.predict_proba(test_scaled[0:1])[0, 1]
```

---

### 07_visualization_ready/

Pre-processed data for interactive 3D visualization.

#### Files Generated

**point_cloud.csv** - Complete data for each point
- Implementation: `data_exporter.py` lines 577-586
- All coordinates, outcomes, categories, metadata
- Ready for Three.js visualization

**category_labels.csv** - High/low category assignments
- Implementation: `data_exporter.py` lines 588-597
- Categories: both_high, first_high, second_high, both_low, middle
- Based on 80th/20th percentile thresholds

**thresholds.csv** - Thresholds used for categorization
- Implementation: `data_exporter.py` lines 599-609
- Contains low (20th) and high (80th) percentiles per outcome

#### Category Definitions

**Implementation:** `pipeline/data_loader.py` lines 250-295

```python
def get_category_assignments(self) -> Tuple[np.ndarray, Dict[str, np.ndarray]]:
    # Calculate thresholds
    for outcome in self.config.data.outcomes:
        low_threshold = np.percentile(self.data[outcome.name], 20)
        high_threshold = np.percentile(self.data[outcome.name], 80)
    
    # Assign categories
    both_high = (outcomes[0] > high_0) & (outcomes[1] > high_1)
    first_high = (outcomes[0] > high_0) & (outcomes[1] < low_1)
    # etc...
```

---

## Complete Algorithm Documentation

### 1. Text Embedding Generation

**Model:** {config.embedding_model}
**Implementation:** `pipeline/embeddings.py` lines 50-108

**Process:**
1. Load SentenceTransformer model (line 31)
2. Set device to CUDA if available (line 35)
3. Process in batches of {config.analysis.batch_size} (line 71)
4. Apply L2 normalization (line 77)
5. Validate all embeddings are unit vectors (lines 110-140)

**Parameters:**
- Max sequence length: {config.analysis.max_text_length} tokens
- Output dimension: {pipeline.embedding_gen.embeddings.shape[1] if hasattr(pipeline, 'embedding_gen') and hasattr(pipeline.embedding_gen, 'embeddings') and pipeline.embedding_gen.embeddings is not None else 'N/A'}
- Pooling: Model-specific (likely CLS token or mean pooling)

### 2. Principal Component Analysis (PCA)

**Implementation:** `pipeline/dimensionality.py` lines 31-75
**Scikit-learn version:** {versions.get('scikit-learn', 'unknown')}

**Preprocessing:**
```python
self.scaler = StandardScaler()
embeddings_scaled = self.scaler.fit_transform(embeddings)
```

**PCA Configuration:**
```python
self.pca = PCA(
    n_components={config.analysis.pca_components},
    random_state=42
)
```

**Percentile Calculation:**
```python
percentiles[:, i] = rankdata(pca_features[:, i], 'average') / len(pca_features) * 100
```

### 3. UMAP (Uniform Manifold Approximation and Projection)

**Implementation:** `pipeline/dimensionality.py` lines 77-120
**UMAP version:** {versions.get('umap-learn', 'unknown')}

**Configuration:**
```python
self.umap_model = umap.UMAP(
    n_components={config.analysis.umap_dimensions},
    n_neighbors={config.analysis.umap_n_neighbors},
    min_dist={config.analysis.umap_min_dist},
    metric='cosine',
    random_state=42,
    verbose=True
)
```

**Normalization to [-1, 1]:**
```python
for i in range(umap_embeddings.shape[1]):
    coord_min = umap_embeddings[:, i].min()
    coord_max = umap_embeddings[:, i].max()
    normalized[:, i] = 2 * (umap_embeddings[:, i] - coord_min) / (coord_max - coord_min) - 1
```

### 4. HDBSCAN Clustering

**Implementation:** `pipeline/clustering.py` lines 33-72

**Configuration:**
```python
self.clustering_model = hdbscan.HDBSCAN(
    min_cluster_size={config.analysis.hdbscan_min_cluster_size},
    min_samples={config.analysis.hdbscan_min_samples},
    metric='euclidean',
    cluster_selection_epsilon=0.0,
    cluster_selection_method='eom'
)
```

**Noise Handling:**
- Points not in any cluster assigned label -1
- Stored in cluster_assignments.csv

### 5. Topic Keyword Extraction (c-TF-IDF)

**Implementation:** `pipeline/clustering.py` lines 74-125

**Process:**
1. Concatenate all texts per cluster
2. Apply TF-IDF vectorization:
```python
tfidf = TfidfVectorizer(
    max_features=100,
    stop_words='english',
    ngram_range=(1, 2)
)
```
3. Select top 10 keywords by score
4. Format as hyphen-separated string

### 6. Feature Importance Analysis

#### 6.1 Mutual Information
**Implementation:** `pipeline/dimensionality.py` lines 180-190

```python
mi_scores = mutual_info_regression(
    pca_features, 
    combined_outcome,
    n_neighbors=3,
    random_state=42
)
```

#### 6.2 XGBoost Feature Importance
**Implementation:** `pipeline/dimensionality.py` lines 193-213
**XGBoost version:** {versions.get('xgboost', 'unknown')}

```python
xgb_model = xgb.XGBRegressor(
    n_estimators={config.analysis.xgb_n_estimators},
    max_depth=6,  # Fixed for feature selection
    random_state=42,
    tree_method='hist',
    device='cuda'
)
importance_scores = xgb_model.feature_importances_
```

### 7. Double Machine Learning (DML)

**Implementation:** `pipeline/dml_analysis.py`

#### 7.1 Cross-Fitting Procedure
**K-folds:** {config.analysis.dml_n_folds}
**Implementation:** Lines 189-285

```python
kf = KFold(n_splits={config.analysis.dml_n_folds}, shuffle=True, random_state=42)

for fold_idx, (train_idx, test_idx) in enumerate(kf.split(X)):
    # Train nuisance models on training fold
    model_outcome.fit(X_train, outcome_train)
    model_treatment.fit(X_train, treatment_train)
    
    # Predict on test fold
    residuals_outcome_all[test_idx] = outcome_test - model_outcome.predict(X_test)
    residuals_treatment_all[test_idx] = treatment_test - model_treatment.predict(X_test)
```

#### 7.2 Nuisance Models
**Implementation:** Lines 213-229

```python
model_outcome = xgb.XGBRegressor(
    n_estimators={config.analysis.xgb_n_estimators},
    max_depth={config.analysis.xgb_max_depth},
    random_state=42,
    tree_method='hist',
    device='cuda'
)
```

#### 7.3 DML Estimator
**Implementation:** Line 247

```python
theta = np.sum(residuals_treatment_all * residuals_outcome_all) / np.sum(residuals_treatment_all ** 2)
```

#### 7.4 Standard Error
**Implementation:** Lines 250-252

```python
se = np.sqrt(np.sum((residuals_outcome_all - theta * residuals_treatment_all) ** 2) / 
             (n * np.sum(residuals_treatment_all ** 2)))
```

### 8. PC Effect Analysis

**Implementation:** `run_pipeline.py` lines 282-337

**Process:**
1. Standardize all PCs
2. For each PC:
   - Create test points at 90th and 10th percentiles
   - Fit logistic regression for high/low outcomes
   - Calculate probabilities at test points
   - Store differences

**Models:**
```python
model_high = LogisticRegression(max_iter=1000, random_state=42)
model_low = LogisticRegression(max_iter=1000, random_state=42)
```

### 9. PC-Topic Associations

**Implementation:** `run_pipeline.py` lines 504-556

**Statistical Test:** Welch's t-test (unequal variances)
```python
t_stat, p_value = stats.ttest_ind(topic_pc_values, non_topic_pc_values, equal_var=False)
```

**Metrics Calculated:**
- Average percentile of PC values in topic
- Standard deviation of percentiles
- T-statistic and p-value
- Mean PC values for topic vs non-topic

---

## Data Processing Pipeline

### Execution Order

1. **Data Loading** (`data_loader.py`)
   - Load CSV file
   - Apply sampling if configured
   - Validate outcome columns

2. **Embedding Generation** (`embeddings.py`)
   - Load pre-trained model
   - Process texts in batches
   - Normalize to unit vectors

3. **PCA** (`dimensionality.py`)
   - Standardize embeddings
   - Fit PCA with {config.analysis.pca_components} components
   - Calculate percentiles

4. **UMAP** (`dimensionality.py`)
   - Use PCA features as input
   - Reduce to {config.analysis.umap_dimensions}D
   - Normalize to [-1, 1]

5. **Clustering** (`clustering.py`)
   - Apply HDBSCAN on UMAP coordinates
   - Extract topic keywords

6. **DML Analysis** (`dml_analysis.py`)
   - Run for three feature sets:
     - Raw embeddings
     - All 200 PCs
     - Top 5 PCs
   - Export residuals and predictions for each

7. **Visualization Prep** (`run_pipeline.py`)
   - Calculate thresholds
   - Assign categories
   - Compute PC effects
   - Generate statistics

8. **Export** (`data_exporter.py`)
   - Save all intermediate results
   - Generate this documentation

---

## Model Training Details

### XGBoost Configuration

**For Feature Selection:**
```python
xgb.XGBRegressor(
    n_estimators={config.analysis.xgb_n_estimators},
    max_depth=6,  # Fixed for selection
    random_state=42,
    tree_method='hist',
    device='cuda'
)
```

**For DML Nuisance Models:**
```python
xgb.XGBRegressor(
    n_estimators={config.analysis.xgb_n_estimators},
    max_depth={config.analysis.xgb_max_depth},
    random_state=42,
    tree_method='hist',
    device='cuda'
)
```

**Additional Parameters (defaults):**
- learning_rate: 0.3
- subsample: 1.0
- colsample_bytree: 1.0
- reg_alpha (L1): 0
- reg_lambda (L2): 1

### Logistic Regression (PC Effects)

```python
LogisticRegression(
    max_iter=1000,
    random_state=42,
    solver='lbfgs',  # Default
    C=1.0  # Default regularization
)
```

---

## Export Implementation Details

### Export Process

**Entry Point:** `run_pipeline.py` line 678
```python
export_path = pipeline.data_exporter.export_all_to_csv(
    pipeline=pipeline,
    results=results,
    output_dir=export_dir,
    exclude_text=exclude_text,
    anonymize_ids=anonymize_ids,
    cli_command=getattr(pipeline, '_cli_command', None)
)
```

### Directory Creation

**Implementation:** `data_exporter.py` lines 56-67
```python
dirs = {{
    'raw': output_dir / '01_raw_data',
    'dim_reduction': output_dir / '02_dimensionality_reduction',
    'clustering': output_dir / '03_clustering',
    'importance': output_dir / '04_feature_importance',
    'dml': output_dir / '05_dml_analysis',
    'pc_analysis': output_dir / '06_pc_analysis',
    'viz': output_dir / '07_visualization_ready'
}}
```

### Error Handling

**Implementation:** `data_exporter.py` lines 69-141

Each export section wrapped in try/except:
```python
try:
    self._export_raw_data(dirs['raw'], pipeline, exclude_text, anonymize_ids)
    print("    ✓ Complete")
except Exception as e:
    print(f"    ⚠️  Error: {{str(e)}}")
    export_warnings.append(f"Raw data export: {{str(e)}}")
```

Warnings collected and written to `export_warnings.txt` if any occur.

### Multi-Model Residual Export

**Implementation:** Modified in lines 91-96 of `dml_analysis.py`
```python
# Store with model suffix for multi-model export
storage_key = f"{{pair_key}}_{{model_suffix}}" if model_suffix else pair_key
if 'residuals' in dml_result:
    self.residuals[storage_key] = dml_result['residuals']
if 'predictions' in dml_result:
    self.predictions[storage_key] = dml_result['predictions']
```

**Model Suffixes:**
- `embeddings` - Raw embedding features
- `200pcs` - All PCA components
- `top5pcs` - Selected top 5 PCs

---

## Reproducibility Guide

### Exact Reproduction Requirements

1. **Python Environment**
   ```bash
   python -m venv perceptionml_env
   source perceptionml_env/bin/activate  # Linux/Mac
   # or
   perceptionml_env\\Scripts\\activate  # Windows
   ```

2. **Package Versions**
   ```bash
   pip install numpy=={versions.get('numpy', 'X.X.X')}
   pip install pandas=={versions.get('pandas', 'X.X.X')}
   pip install scikit-learn=={versions.get('scikit-learn', 'X.X.X')}
   pip install umap-learn=={versions.get('umap-learn', 'X.X.X')}
   pip install xgboost=={versions.get('xgboost', 'X.X.X')}
   pip install shap=={versions.get('shap', 'X.X.X')}
   pip install torch=={versions.get('torch', 'X.X.X')}
   pip install transformers=={versions.get('transformers', 'X.X.X')}
   pip install plotly=={versions.get('plotly', 'X.X.X')}
   pip install jinja2=={versions.get('jinja2', 'X.X.X')}
   ```

3. **Random Seeds Used**
   - General: 42 (throughout pipeline)
   - NumPy: Set in data_loader.py if sampling
   - Python random: Set in data_loader.py if sampling
   - All models initialized with random_state=42

### Reproducing DML Results from Exports

```python
import pandas as pd
import numpy as np

# Load residuals
residuals = pd.read_csv('05_dml_analysis/residuals/residuals_X_to_Y_200pcs.csv')

# Reproduce DML estimate
theta = np.sum(residuals['residual_treatment'] * residuals['residual_outcome']) / \\
        np.sum(residuals['residual_treatment'] ** 2)

# Calculate standard error
n = len(residuals)
se = np.sqrt(np.sum((residuals['residual_outcome'] - theta * residuals['residual_treatment']) ** 2) / 
             (n * np.sum(residuals['residual_treatment'] ** 2)))

print(f"Theta: {{theta:.4f}}, SE: {{se:.4f}}")
```

### Verifying Predictions Match Residuals

```python
# Load predictions and original data
predictions = pd.read_csv('05_dml_analysis/predictions/predictions_X_to_Y_200pcs.csv')
original = pd.read_csv('01_raw_data/original_data.csv')

# Merge by ID
merged = predictions.merge(original, on='{config.data.id_column}')

# Calculate residuals
calc_residual_Y = merged['Y'] - merged['predicted_outcome']
calc_residual_X = merged['X'] - merged['predicted_treatment']

# Load exported residuals
residuals = pd.read_csv('05_dml_analysis/residuals/residuals_X_to_Y_200pcs.csv')
merged_resid = merged.merge(residuals, on='{config.data.id_column}')

# Verify they match
print(f"Y residuals match: {{np.allclose(calc_residual_Y, merged_resid['residual_outcome'])}}")
print(f"X residuals match: {{np.allclose(calc_residual_X, merged_resid['residual_treatment'])}}")
```

### Loading State File

To reload the complete pipeline state:

```bash
python run_pipeline.py --import-state [state_file.pkl] -o new_visualization.html
```

**Implementation:** `run_pipeline.py` lines 613-638

---

## Code References

### Core Pipeline Files

1. **run_pipeline.py** (702 lines)
   - Main orchestrator
   - Pipeline class: lines 27-566
   - CLI interface: lines 569-698

2. **pipeline/config.py** (186 lines)
   - Configuration dataclasses
   - Validation logic
   - YAML parsing

3. **pipeline/data_loader.py** (350 lines)
   - CSV loading
   - Sampling implementation
   - Threshold calculation
   - Category assignment

4. **pipeline/embeddings.py** (150 lines)
   - SentenceTransformer integration
   - Batch processing
   - Normalization
   - Validation

5. **pipeline/dimensionality.py** (230 lines)
   - PCA implementation
   - UMAP implementation
   - PC selection methods

6. **pipeline/clustering.py** (380 lines)
   - HDBSCAN clustering
   - Topic extraction
   - Statistics calculation

7. **pipeline/dml_analysis.py** (377 lines)
   - DML estimation
   - Cross-fitting
   - Residual calculation
   - Multi-model support

8. **pipeline/visualization.py** (280 lines)
   - HTML generation
   - Jinja2 templating
   - JavaScript integration

9. **pipeline/data_exporter.py** (1231 lines)
   - CSV export logic
   - State serialization
   - This documentation

### Key Functions

**Data Loading:**
- `DataLoader.load_data()` - lines 50-115
- `DataLoader._apply_sampling()` - lines 150-207

**Embedding Generation:**
- `EmbeddingGenerator.generate_embeddings()` - lines 50-108
- `EmbeddingGenerator.validate_embeddings()` - lines 110-140

**Dimensionality Reduction:**
- `DimensionalityReducer.fit_pca()` - lines 31-75
- `DimensionalityReducer.fit_umap()` - lines 77-120
- `DimensionalityReducer.select_top_pcs_for_dml()` - lines 140-225

**Clustering:**
- `TopicModeler.fit_clusters()` - lines 33-72
- `TopicModeler.extract_topics()` - lines 74-125
- `TopicModeler.calculate_extreme_group_statistics()` - lines 276-363

**DML Analysis:**
- `DMLAnalyzer.run_dml_analysis()` - lines 26-102
- `DMLAnalyzer._estimate_dml_crossfit()` - lines 189-285
- `DMLAnalyzer._estimate_naive()` - lines 104-130

**Export Functions:**
- `DataExporter.export_all_to_csv()` - lines 25-147
- `DataExporter._export_dml_analysis()` - lines 412-520
- `DataExporter._generate_readme()` - lines 611-1111

---

## Notes on Potential Variability

Even with fixed random seeds, results may vary slightly due to:

1. **Floating-point precision**
   - Different CPU architectures
   - BLAS/LAPACK implementations

2. **GPU vs CPU execution**
   - XGBoost tree construction
   - CUDA version differences

3. **Library version differences**
   - Algorithm improvements
   - Bug fixes affecting numerics

4. **Operating system differences**
   - Thread scheduling
   - Memory allocation

For exact reproduction, use the saved state file:
```bash
python run_pipeline.py --import-state [state_file]
```

---

## Questions or Issues?

For questions about this export or the PerceptionML pipeline:
1. Check the main documentation
2. Review the code references above
3. Contact the maintainers

**Generated by:** PerceptionML v{getattr(pipeline, 'version', '1.0.0')}
**Export version:** 1.0.0
"""
    
    return readme_content